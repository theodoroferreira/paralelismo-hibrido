{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Pi Estimation - Hybrid MPI + CUDA\n",
    "\n",
    "This notebook demonstrates hybrid parallel computing using:\n",
    "- **MPI (mpi4py)**: For multi-process parallelism\n",
    "- **CUDA (numba)**: For GPU acceleration\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "**Important**: Enable GPU in Google Colab:\n",
    "1. Go to `Runtime` → `Change runtime type`\n",
    "2. Set `Hardware accelerator` to `GPU`\n",
    "3. Click `Save`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MPI and Python packages\n",
    "!apt-get -qq install -y openmpi-bin libopenmpi-dev\n",
    "!pip install -q mpi4py numba\n",
    "\n",
    "print(\"✓ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Upload the Python Script\n",
    "\n",
    "Upload `monte_carlo_pi_colab.py` to Colab, or create it directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile monte_carlo_pi_colab.py\n",
    "\"\"\"\n",
    "Monte Carlo Pi Estimation - Hybrid MPI + CUDA Implementation for Google Colab\n",
    "\n",
    "This program estimates the value of π using the Monte Carlo method\n",
    "with hybrid parallelism combining mpi4py (inter-node) and CUDA (GPU acceleration).\n",
    "\n",
    "Method:\n",
    "1. Generate random points in the unit square [0,1] × [0,1]\n",
    "2. Count points falling inside the unit circle (x² + y² ≤ 1)\n",
    "3. Estimate π ≈ 4 × (points_inside / total_points)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Try to import MPI, but make it optional for single-process runs\n",
    "try:\n",
    "    from mpi4py import MPI\n",
    "    MPI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MPI_AVAILABLE = False\n",
    "    print(\"Warning: mpi4py not available, running in single-process mode\")\n",
    "\n",
    "\n",
    "# CUDA kernel for Monte Carlo simulation\n",
    "@cuda.jit\n",
    "def monte_carlo_kernel(points_inside, total_points_per_thread, seed_base):\n",
    "    \"\"\"\n",
    "    CUDA kernel to generate random points and count those inside the unit circle.\n",
    "\n",
    "    Each thread generates random points and counts how many fall inside the circle.\n",
    "    Uses a simple LCG (Linear Congruential Generator) for random numbers.\n",
    "\n",
    "    Args:\n",
    "        points_inside: Output array to store count of points inside circle per thread\n",
    "        total_points_per_thread: Number of points each thread should generate\n",
    "        seed_base: Base seed for random number generation\n",
    "    \"\"\"\n",
    "    # Get thread index\n",
    "    idx = cuda.grid(1)\n",
    "\n",
    "    if idx < points_inside.size:\n",
    "        # Initialize random seed for this thread\n",
    "        # LCG parameters (same as glibc)\n",
    "        seed = seed_base + idx\n",
    "        count = 0\n",
    "\n",
    "        # Generate and test points\n",
    "        for i in range(total_points_per_thread):\n",
    "            # Linear Congruential Generator for random numbers\n",
    "            # x_n+1 = (a * x_n + c) mod m\n",
    "            seed = (1103515245 * seed + 12345) & 0x7fffffff\n",
    "            x = (seed & 0xFFFF) / 65536.0  # Random x in [0,1]\n",
    "\n",
    "            seed = (1103515245 * seed + 12345) & 0x7fffffff\n",
    "            y = (seed & 0xFFFF) / 65536.0  # Random y in [0,1]\n",
    "\n",
    "            # Check if point is inside unit circle\n",
    "            if x * x + y * y <= 1.0:\n",
    "                count += 1\n",
    "\n",
    "        # Store result\n",
    "        points_inside[idx] = count\n",
    "\n",
    "\n",
    "def monte_carlo_cuda(total_points, rank=0, size=1):\n",
    "    \"\"\"\n",
    "    Run Monte Carlo simulation on GPU using CUDA.\n",
    "\n",
    "    Args:\n",
    "        total_points: Total number of points to generate\n",
    "        rank: MPI rank (process ID)\n",
    "        size: Total number of MPI processes\n",
    "\n",
    "    Returns:\n",
    "        Number of points inside the circle for this process\n",
    "    \"\"\"\n",
    "    # Check if CUDA is available\n",
    "    if not cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA is not available. Please enable GPU in Colab: Runtime → Change runtime type → GPU\")\n",
    "\n",
    "    # Distribute work among MPI processes\n",
    "    points_per_process = total_points // size\n",
    "    remainder = total_points % size\n",
    "\n",
    "    # First 'remainder' processes get one extra point\n",
    "    if rank < remainder:\n",
    "        points_per_process += 1\n",
    "\n",
    "    # CUDA configuration\n",
    "    threads_per_block = 256\n",
    "    blocks = min((points_per_process + threads_per_block - 1) // threads_per_block, 65535)\n",
    "    total_threads = threads_per_block * blocks\n",
    "\n",
    "    # Distribute points among CUDA threads\n",
    "    points_per_thread = points_per_process // total_threads\n",
    "    thread_remainder = points_per_process % total_threads\n",
    "\n",
    "    # Allocate device memory\n",
    "    d_points_inside = cuda.device_array(total_threads, dtype=np.int64)\n",
    "\n",
    "    # Generate unique seed for this process (ensure it fits in 32-bit range)\n",
    "    seed_base = (int(time.time() * 1000) + rank * 100000) & 0x7FFFFFFF\n",
    "\n",
    "    # Launch CUDA kernel\n",
    "    monte_carlo_kernel[blocks, threads_per_block](d_points_inside, points_per_thread, seed_base)\n",
    "\n",
    "    # Copy results back to host\n",
    "    h_points_inside = d_points_inside.copy_to_host()\n",
    "\n",
    "    # Handle remainder points (run on CPU to avoid complexity)\n",
    "    cpu_points = thread_remainder\n",
    "    if cpu_points > 0:\n",
    "        # Ensure seed is within NumPy's acceptable range [0, 2^32 - 1]\n",
    "        cpu_seed = (seed_base + total_threads) % (2**32)\n",
    "        np.random.seed(cpu_seed)\n",
    "        x = np.random.random(cpu_points)\n",
    "        y = np.random.random(cpu_points)\n",
    "        cpu_count = np.sum(x*x + y*y <= 1.0)\n",
    "    else:\n",
    "        cpu_count = 0\n",
    "\n",
    "    # Sum all counts\n",
    "    local_count = np.sum(h_points_inside) + cpu_count\n",
    "\n",
    "    return local_count\n",
    "\n",
    "\n",
    "def print_gpu_info():\n",
    "    \"\"\"Print information about available GPU.\"\"\"\n",
    "    if cuda.is_available():\n",
    "        device = cuda.get_current_device()\n",
    "        print(f\"GPU Device: {device.name.decode()}\")\n",
    "        print(f\"Compute Capability: {device.compute_capability}\")\n",
    "        print(f\"Total Memory: {device.total_memory / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        print(\"No GPU available\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run hybrid MPI+CUDA Monte Carlo simulation.\"\"\"\n",
    "\n",
    "    # Initialize MPI if available\n",
    "    if MPI_AVAILABLE:\n",
    "        comm = MPI.COMM_WORLD\n",
    "        rank = comm.Get_rank()\n",
    "        size = comm.Get_size()\n",
    "    else:\n",
    "        rank = 0\n",
    "        size = 1\n",
    "        comm = None\n",
    "\n",
    "    # Default number of points\n",
    "    total_points = 100000000  # 10^8\n",
    "\n",
    "    # Parse command line arguments\n",
    "    if len(sys.argv) > 1:\n",
    "        try:\n",
    "            total_points = int(sys.argv[1])\n",
    "            if total_points <= 0:\n",
    "                if rank == 0:\n",
    "                    print(\"Error: Number of points must be positive\", file=sys.stderr)\n",
    "                sys.exit(1)\n",
    "        except ValueError:\n",
    "            if rank == 0:\n",
    "                print(\"Error: Invalid number format\", file=sys.stderr)\n",
    "            sys.exit(1)\n",
    "\n",
    "    # Print header (only rank 0)\n",
    "    if rank == 0:\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Monte Carlo Pi Estimation - Hybrid MPI+CUDA (Python)\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Total points: {total_points:,}\")\n",
    "        print(f\"MPI processes: {size}\")\n",
    "\n",
    "        if cuda.is_available():\n",
    "            print_gpu_info()\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "        print()\n",
    "\n",
    "    # Synchronize all processes before timing\n",
    "    if MPI_AVAILABLE:\n",
    "        comm.Barrier()\n",
    "\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Run Monte Carlo simulation on GPU\n",
    "    try:\n",
    "        local_count = monte_carlo_cuda(total_points, rank, size)\n",
    "    except RuntimeError as e:\n",
    "        if rank == 0:\n",
    "            print(f\"Error: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Reduce results from all MPI processes\n",
    "    if MPI_AVAILABLE:\n",
    "        global_count = comm.reduce(local_count, op=MPI.SUM, root=0)\n",
    "    else:\n",
    "        global_count = local_count\n",
    "\n",
    "    # Synchronize and stop timing\n",
    "    if MPI_AVAILABLE:\n",
    "        comm.Barrier()\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate and display results (only rank 0)\n",
    "    if rank == 0:\n",
    "        pi_estimate = 4.0 * global_count / total_points\n",
    "        pi_actual = math.pi\n",
    "        error = abs(pi_estimate - pi_actual)\n",
    "        error_percentage = (error / pi_actual) * 100.0\n",
    "        execution_time = end_time - start_time\n",
    "\n",
    "        print(\"Results:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"Estimated π:        {pi_estimate:.15f}\")\n",
    "        print(f\"Actual π:           {pi_actual:.15f}\")\n",
    "        print(f\"Absolute error:     {error:.15f}\")\n",
    "        print(f\"Relative error:     {error_percentage:.10f}%\")\n",
    "        print(f\"Points inside:      {global_count:,}\")\n",
    "        print(f\"Total points:       {total_points:,}\")\n",
    "        print(f\"Execution time:     {execution_time:.6f} seconds\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Verify GPU is Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "if cuda.is_available():\n",
    "    device = cuda.get_current_device()\n",
    "    print(f\"✓ GPU Available: {device.name.decode()}\")\n",
    "    print(f\"  Compute Capability: {device.compute_capability}\")\n",
    "    print(f\"  Total Memory: {device.total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"✗ No GPU available!\")\n",
    "    print(\"  Please enable GPU: Runtime → Change runtime type → GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Single-Process Version (GPU Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with default settings (100 million points)\n",
    "!python monte_carlo_pi_colab.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run with Custom Number of Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with 1 billion points for higher accuracy\n",
    "!python monte_carlo_pi_colab.py 1000000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run Hybrid MPI + CUDA Version\n",
    "\n",
    "This runs multiple MPI processes, each using the GPU.\n",
    "\n",
    "**Note**: In Colab, you only have access to one GPU, but MPI can still distribute work among multiple CPU processes that share the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with 2 MPI processes (each using GPU)\n",
    "!mpirun -np 2 --allow-run-as-root python monte_carlo_pi_colab.py 1000000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Run with More MPI Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with 4 MPI processes\n",
    "!mpirun -np 4 --allow-run-as-root python monte_carlo_pi_colab.py 1000000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "Let's compare execution times with different configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Run experiments\n",
    "points = 500000000  # 500 million points\n",
    "\n",
    "print(\"Running performance comparison...\\n\")\n",
    "\n",
    "print(\"1 Process (GPU only):\")\n",
    "!python monte_carlo_pi_colab.py {points}\n",
    "\n",
    "print(\"\\n2 MPI Processes:\")\n",
    "!mpirun -np 2 --allow-run-as-root python monte_carlo_pi_colab.py {points}\n",
    "\n",
    "print(\"\\n4 MPI Processes:\")\n",
    "!mpirun -np 4 --allow-run-as-root python monte_carlo_pi_colab.py {points}"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
