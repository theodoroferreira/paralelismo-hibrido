\documentclass[12pt]{article}

\usepackage{sbc-template}
\usepackage{graphicx,url}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
%\usepackage[latin1]{inputenc}  

     
\sloppy

\title{Estimativa de $\pi$ por Monte Carlo com Paralelismo Híbrido MPI+OpenMP: Decisões de Implementação e Análise de Desempenho}

\author{Matheus Gabriel Girardi, Theodoro Gaspar Ferreira}

\address{Curso de Ciência da Computação\\
  Universidade Regional do Noroeste do Estado do Rio Grande do Sul (UNIJUÍ)\\
  Ijuí -- RS -- Brasil
  \email{\{matheus.girardi,theodoro.ferreira\}@sou.unijui.edu.br}
}

\begin{document} 

\maketitle

\begin{abstract}
  This work presents the implementation and analysis of a hybrid algorithm for estimating $\pi$ using the Monte Carlo method with two-level parallelism: MPI for inter-process parallelism and OpenMP for intra-process parallelism. Critical architectural decisions are discussed, including work distribution strategies, thread-safe random number generation, and result synchronization mechanisms. Experiments conducted on a Ryzen 5 7600X processor with 6 MPI processes and 12 OpenMP threads (72 parallel workers) demonstrate efficient scalability, achieving an execution time of 731 seconds for one trillion points with a relative error below 0.0002\%.
\end{abstract}

\begin{resumo}
  Este trabalho apresenta a implementação e análise de um algoritmo híbrido para estimativa de $\pi$ usando o método Monte Carlo com paralelismo em dois níveis: MPI para paralelismo inter-processo e OpenMP para paralelismo intra-processo. São discutidas decisões arquiteturais críticas, incluindo estratégias de distribuição de trabalho, geração de números aleatórios thread-safe e sincronização de resultados. Experimentos realizados em um processador Ryzen 5 7600X com 6 processos MPI e 12 threads OpenMP (72 workers paralelos) demonstram escalabilidade eficiente, alcançando tempo de execução de 731 segundos para 1 trilhão de pontos com erro relativo inferior a 0,0002\%.
\end{resumo}


\section{Introdução}

O método Monte Carlo é uma técnica computacional amplamente utilizada para resolver problemas por meio de amostragem aleatória \cite{metropolis1949}. Uma aplicação clássica deste método é a estimativa do valor de $\pi$ através da geração de pontos aleatórios em um quadrado unitário e a contagem daqueles que caem dentro de um círculo inscrito.

Com o crescimento da disponibilidade de sistemas multicore e clusters de computadores, o paralelismo híbrido emerge como uma abordagem eficiente para explorar diferentes níveis de hierarquia de memória. Este trabalho, desenvolvido no contexto da disciplina de Programação Paralela, implementa e analisa uma solução híbrida combinando MPI (Message Passing Interface) para comunicação entre processos distribuídos e OpenMP para paralelismo de memória compartilhada dentro de cada processo.

O objetivo principal é investigar as decisões arquiteturais críticas na implementação de algoritmos híbridos, com ênfase em três aspectos fundamentais: estratégias de distribuição de trabalho entre processos e threads, geração de números aleatórios de forma thread-safe, e mecanismos eficientes de sincronização e agregação de resultados. Adicionalmente, este trabalho apresenta uma análise detalhada de desempenho e escalabilidade da implementação.

\section{Fundamentação Teórica}

\subsection{Método Monte Carlo para Estimativa de $\pi$}

O método baseia-se na relação geométrica entre a área de um círculo unitário ($A_c = \pi r^2 = \pi/4$ para $r=1/2$) e a área do quadrado que o circunscreve ($A_q = 1$). Gerando $N$ pontos aleatórios uniformemente distribuídos no quadrado $[0,1] \times [0,1]$ e contando quantos caem dentro do círculo ($N_c$), a estimativa de $\pi$ é dada por:

\begin{equation}
\pi \approx 4 \cdot \frac{N_c}{N}
\end{equation}

A precisão da estimativa melhora proporcionalmente a $\sqrt{N}$, tornando essencial o uso de grandes volumes de pontos e, consequentemente, de paralelização eficiente.

\subsection{Paralelismo Híbrido MPI+OpenMP}

O modelo híbrido combina dois paradigmas de programação paralela \cite{rabenseifner2009}:

\textbf{MPI} opera no nível de processos distribuídos, adequado para sistemas com memória distribuída (clusters). Cada processo MPI possui seu próprio espaço de endereçamento e comunica-se explicitamente via troca de mensagens.

\textbf{OpenMP} implementa paralelismo de memória compartilhada através de threads que compartilham o mesmo espaço de endereçamento dentro de um processo. É ideal para explorar múltiplos cores em arquiteturas multicore modernas.

A combinação permite explorar eficientemente arquiteturas hierárquicas, onde MPI distribui trabalho entre nós computacionais (ou processos) e OpenMP paraleliza dentro de cada nó, maximizando o uso de recursos.

\section{Decisões de Implementação}

\subsection{Arquitetura Híbrida de Dois Níveis}

A implementação adota uma arquitetura de paralelismo hierárquico em dois níveis claramente definidos:

\textbf{Nível 1 - MPI (granularidade grossa):} O conjunto total de $N$ pontos é dividido igualmente entre $P$ processos MPI. Cada processo recebe $N/P$ pontos, com o restante da divisão distribuído entre os primeiros processos para garantir que todos os pontos sejam processados. Esta divisão ocorre na linha 85-91 do código fonte.

\textbf{Nível 2 - OpenMP (granularidade fina):} Dentro de cada processo MPI, o trabalho é subdividido entre $T$ threads OpenMP. Cada thread processa $N/(P \cdot T)$ pontos de forma independente, utilizando a diretiva \texttt{\#pragma omp parallel reduction(+:local\_count)} para paralelização automática e agregação thread-safe dos resultados parciais.

Esta estratégia resulta em $P \times T$ workers paralelos executando simultaneamente. Nos experimentos realizados, utilizou-se $P=6$ processos e $T=12$ threads, totalizando 72 workers paralelos.

\subsection{Estratégia de Distribuição de Trabalho}

A distribuição de trabalho implementa um esquema de balanceamento de carga estático com tratamento cuidadoso de restos de divisão:

\begin{enumerate}
\item \textbf{Distribuição entre processos MPI:} Calcula-se \texttt{points\_per\_process = total\_points / size} e \texttt{remainder = total\_points \% size}. Os primeiros \texttt{remainder} processos recebem um ponto adicional, garantindo distribuição uniforme sem desperdício.

\item \textbf{Distribuição entre threads OpenMP:} Cada processo MPI repete o procedimento internamente, dividindo seus pontos entre threads. Esta distribuição hierárquica é determinística e evita sobrecarga de sincronização durante a execução.
\end{enumerate}

A vantagem desta abordagem é a eliminação de balanceamento dinâmico, reduzindo overhead de sincronização. Como o custo computacional por ponto é uniforme (duas gerações de números aleatórios e uma comparação), a distribuição estática é ótima.

\subsection{Geração de Números Aleatórios Thread-Safe}

Um desafio crítico em implementações paralelas de Monte Carlo é garantir qualidade estatística dos números aleatórios sem introduzir contenção entre threads. A solução adotada utiliza:

\textbf{Função rand\_r():} Diferentemente de \texttt{rand()}, que mantém estado global compartilhado, \texttt{rand\_r()} recebe o estado como parâmetro, permitindo que cada thread mantenha seu gerador independente (linha 33-43).

\textbf{Sementes únicas:} Cada thread recebe uma semente única calculada como:
\begin{verbatim}
seed = time(NULL) + rank * 1000 + thread_id
\end{verbatim}

Esta fórmula garante que threads em diferentes processos e dentro do mesmo processo tenham sequências aleatórias descorrelacionadas, essencial para validade estatística do método Monte Carlo.

\textbf{Estado local:} Cada thread mantém \texttt{local\_seed} privado, eliminando qualquer necessidade de sincronização durante a geração de números aleatórios, que representa a operação mais frequente do algoritmo.

\subsection{Sincronização e Agregação de Resultados}

A agregação de resultados segue uma estratégia hierárquica correspondente à arquitetura de dois níveis:

\textbf{Agregação OpenMP:} A diretiva \texttt{reduction(+:local\_count)} implementa redução automática e otimizada. O compilador OpenMP garante que cada thread acumule seu contador privado sem contenção, realizando a soma final de forma eficiente ao término da região paralela.

\textbf{Sincronização MPI:} Após o cálculo local, utiliza-se \texttt{MPI\_Barrier()} para sincronização temporal antes da medição de tempo (linha 105). A agregação global emprega \texttt{MPI\_Reduce()} com operação \texttt{MPI\_SUM} para coletar todos os contadores locais no processo raiz (linha 131).

\textbf{Inicialização thread-safe:} O programa utiliza \texttt{MPI\_Init\_thread()} com nível \texttt{MPI\_THREAD\_FUNNELED}, garantindo que chamadas MPI sejam seguras em ambiente multi-threaded, embora apenas a thread principal realize comunicação MPI.

Esta arquitetura minimiza comunicação e sincronização: OpenMP realiza agregação local sem comunicação de rede, e MPI executa apenas uma operação de redução global ao final, maximizando eficiência.

\section{Análise de Desempenho e Escalabilidade}

\subsection{Ambiente Experimental}

Os experimentos foram realizados em um processador AMD Ryzen 5 7600X com 6 núcleos físicos (12 threads lógicos via SMT), 32GB de RAM DDR5-5800MHz, executando Linux. A configuração utilizou 6 processos MPI com 12 threads OpenMP cada, totalizando 72 workers paralelos.

\subsection{Resultados Experimentais}

A Tabela~\ref{tab:results} apresenta os resultados para diferentes volumes de pontos, variando de $10^6$ a $10^{12}$.

\begin{table}[ht]
\centering
\caption{Resultados experimentais de desempenho e precisão}
\label{tab:results}
\small
\begin{tabular}{|r|r|r|r|}
\hline
\textbf{Pontos} & \textbf{Tempo (s)} & \textbf{Erro Relativo} & \textbf{Throughput} \\
\hline
$10^6$ & 0,037 & 0,0002338435\% & 27,1 MP/s \\
$10^8$ & 0,102 & 0,0048900805\% & 978,6 MP/s \\
$10^9$ & 0,778 & 0,0024851595\% & 1.285,3 MP/s \\
$10^{10}$ & 7,086 & 0,0002899291\% & 1.411,2 MP/s \\
$10^{11}$ & 71,007 & 0,0001372307\% & 1.408,4 MP/s \\
$10^{12}$ & 731,822 & 0,0001587808\% & 1.366,7 MP/s \\
\hline
\end{tabular}
\end{table}

\subsection{Discussão}

\textbf{Escalabilidade:} O throughput estabiliza em aproximadamente 1,4 bilhões de pontos por segundo para cargas grandes ($\geq 10^9$ pontos), indicando excelente utilização dos recursos. A variação mínima entre $10^9$ e $10^{12}$ pontos (1.285-1.411 MP/s) demonstra escalabilidade eficiente sem degradação significativa.

\textbf{Precisão:} Como esperado teoricamente, o erro relativo diminui com o aumento de pontos, alcançando 0,0001\% para $10^{11}$ pontos. A convergência segue aproximadamente $O(1/\sqrt{N})$, confirmando a validade estatística da implementação.

\textbf{Eficiência:} O baixo overhead evidenciado pela estabilidade do throughput indica que as decisões de implementação (distribuição estática, geração local de aleatórios, redução hierárquica) minimizam contenção e comunicação. O overhead de inicialização torna-se negligível para cargas grandes.

\textbf{Balanceamento:} A distribuição estática com tratamento de restos garante carga equilibrada. Com 72 workers e distribuição uniforme, não há workers ociosos, maximizando eficiência computacional.

\section{Conclusão}

Este trabalho apresentou a implementação e análise de um algoritmo híbrido MPI+OpenMP para estimativa de $\pi$ via método Monte Carlo, com foco em decisões arquiteturais críticas. As principais contribuições incluem:

\begin{enumerate}
\item Arquitetura hierárquica de dois níveis que explora eficientemente memória distribuída (MPI) e compartilhada (OpenMP);
\item Estratégia de distribuição estática com tratamento correto de restos, garantindo balanceamento perfeito;
\item Implementação thread-safe de geração de números aleatórios usando \texttt{rand\_r()} com sementes únicas por thread;
\item Esquema de agregação hierárquica minimizando comunicação e sincronização.
\end{enumerate}

Os resultados experimentais demonstram escalabilidade eficiente, com throughput estável de 1,4 bilhões de pontos/segundo e erro relativo inferior a 0,0002\% para 1 trilhão de pontos. A implementação alcança excelente utilização dos 72 workers paralelos sem degradação significativa de desempenho.

Como trabalhos futuros, sugere-se investigar geradores de números aleatórios mais sofisticados (e.g., Mersenne Twister), explorar diferentes configurações de processos/threads para análise de escalabilidade forte e fraca, e comparar o desempenho com implementações GPU utilizando CUDA.

\bibliographystyle{sbc}
\bibliography{sbc-template}

\end{document}
